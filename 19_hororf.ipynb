{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HoroRF\n",
    "> Comparing two hyperbolic RF methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m2023-09-18 09:03:44,089 [hororf.rf_trainer]\u001b[0m 1 GPUs available\n",
      "\u001b[32;1m2023-09-18 09:03:44,089 [hororf.rf_trainer]\u001b[0m Using seed 17 on class 2\n",
      "\u001b[32;1m2023-09-18 09:03:44,094 [hororf.utils]\u001b[0m 977 datapoints in dataset 'datasets.polblogs_geomstats'\n",
      "\u001b[32;1m2023-09-18 09:03:44,094 [hororf.utils]\u001b[0m 977 datapoints in test dataset 'datasets.polblogs_geomstats'\n",
      "\u001b[32;1m2023-09-18 09:03:44,095 [hororf.rf_trainer]\u001b[0m 781 train and 196 test samples for fold 0\n",
      "\u001b[32;1m2023-09-18 09:06:06,657 [hororf.rf_trainer]\u001b[0m Hyperbolic tree f1 micro: 0.8878, f1 macro: 0.8870, AUPR: 0.0000. Mean depth of 6.00\n",
      "\u001b[32;1m2023-09-18 09:06:06,658 [hororf.rf_trainer]\u001b[0m 781 train and 196 test samples for fold 1\n",
      "\u001b[32;1m2023-09-18 09:08:48,420 [hororf.rf_trainer]\u001b[0m Hyperbolic tree f1 micro: 0.9337, f1 macro: 0.9332, AUPR: 0.0000. Mean depth of 6.00\n",
      "\u001b[32;1m2023-09-18 09:08:48,421 [hororf.rf_trainer]\u001b[0m 782 train and 195 test samples for fold 2\n",
      "\u001b[32;1m2023-09-18 09:11:20,500 [hororf.rf_trainer]\u001b[0m Hyperbolic tree f1 micro: 0.9385, f1 macro: 0.9384, AUPR: 0.0000. Mean depth of 6.00\n",
      "\u001b[32;1m2023-09-18 09:11:20,501 [hororf.rf_trainer]\u001b[0m 782 train and 195 test samples for fold 3\n",
      "\u001b[32;1m2023-09-18 09:13:49,974 [hororf.rf_trainer]\u001b[0m Hyperbolic tree f1 micro: 0.8974, f1 macro: 0.8974, AUPR: 0.0000. Mean depth of 6.00\n",
      "\u001b[32;1m2023-09-18 09:13:49,975 [hororf.rf_trainer]\u001b[0m 782 train and 195 test samples for fold 4\n",
      "\u001b[32;1m2023-09-18 09:16:31,936 [hororf.rf_trainer]\u001b[0m Hyperbolic tree f1 micro: 0.9385, f1 macro: 0.9383, AUPR: 0.0000. Mean depth of 6.00\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Run HoroRF:at 1000 \n",
    "cd HoroRF\n",
    "/home/phil/mambaforge/envs/hdt/bin/python train_hyp_rf.py -h\n",
    "mv ./logs/output ./logs/output_$(date +%Y%m%d_%H%M%S)_hororf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hrf 99.79 0.25\n",
      "results_micro 98.66 0.90\n",
      "rf 99.79 0.25\n"
     ]
    }
   ],
   "source": [
    "# For using hororf outputs\n",
    "# vals = [\n",
    "#     0.8878, 0.9337, 0.9385, 0.8974, 0.9385\n",
    "# ]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "dim = 16\n",
    "dataname = \"gaussian\"\n",
    "# dataname = \"neuroseed\"\n",
    "\n",
    "for suffix in [\"hrf\", \"results_micro\", \"rf\"]:\n",
    "    vals = np.loadtxt(f\"./HoroRF/logs/big_bench/hororf_{dataname}_{dim}/{suffix}.txt\", delimiter=\"\\t\")\n",
    "    print(suffix, f\"{np.mean(vals) * 100:.2f}\", f\"{np.std(vals)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using loader from file: datasets.neuroseed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 28.18it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 28.01it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 30.22it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 28.65it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 29.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperbolic: 0.867 +/- 0.012\n",
      "Euclidean: 0.885 +/- 0.016\n"
     ]
    }
   ],
   "source": [
    "# For 16-dimensional embeddings, HoroRF had a micro-F1 score of 0.675. Let's try ours:\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from src.hyperdt.forest import HyperbolicRandomForestClassifier\n",
    "from src.hyperdt.conversions import convert\n",
    "\n",
    "# Read params from yml file\n",
    "\n",
    "\n",
    "def evaluate_hdt():\n",
    "    params = yaml.safe_load(open(\"HoroRF/params.yml\", \"r\"))\n",
    "\n",
    "    # Dataset\n",
    "    print(f\"Using loader from file: {params['dataset_file']}\")\n",
    "    print()  # For tqdm compatibility\n",
    "    if params[\"dataset_file\"] == \"datasets.gaussian\":\n",
    "        from HoroRF.datasets.gaussian import get_training_data, get_testing_data\n",
    "    elif params[\"dataset_file\"] == \"datasets.neuroseed\":\n",
    "        from HoroRF.datasets.neuroseed import get_training_data, get_testing_data\n",
    "    elif params[\"dataset_file\"] == \"datasets.polblogs_geomstats\":\n",
    "        from HoroRF.datasets.polblogs_geomstats import get_training_data, get_testing_data\n",
    "\n",
    "    # Get data\n",
    "    X_train, y_train = get_training_data(class_label=params[\"class_label\"], seed=params[\"seed\"])\n",
    "    X_train = convert(X_train.numpy(), \"poincare\", \"hyperboloid\")\n",
    "    X_test, y_test = get_testing_data(class_label=params[\"class_label\"], seed=params[\"seed\"])\n",
    "    X_test = convert(X_test.numpy(), \"poincare\", \"hyperboloid\")\n",
    "\n",
    "    # Hyperparams\n",
    "    args = {\n",
    "        \"n_estimators\": params[\"num_trees\"],\n",
    "        \"max_depth\": params[\"max_depth\"],\n",
    "        \"min_samples_leaf\": params[\"min_samples_leaf\"],\n",
    "    }\n",
    "\n",
    "    # 5-fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=params[\"seed\"])\n",
    "    f1_scores_hrf = []\n",
    "    f1_scores_rf = []\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # Hyperbolic\n",
    "        hrf = HyperbolicRandomForestClassifier(**args)\n",
    "        hrf.fit(X_train[train_index], y_train[train_index], use_tqdm=True, seed=params[\"seed\"])\n",
    "        y_pred = hrf.predict(X_train[test_index])\n",
    "        f1_scores_hrf.append(f1_score(y_train[test_index], y_pred, average=\"micro\"))\n",
    "\n",
    "        # Euclidean\n",
    "        rf = RandomForestClassifier(**args, random_state=params[\"seed\"])\n",
    "        rf.fit(X_train[train_index], y_train[train_index])\n",
    "        y_pred = rf.predict(X_train[test_index])\n",
    "        f1_scores_rf.append(f1_score(y_train[test_index], y_pred, average=\"micro\"))\n",
    "\n",
    "    return f1_scores_hrf, f1_scores_rf\n",
    "\n",
    "\n",
    "f1_scores_hrf, f1_scores_rf = evaluate_hdt()\n",
    "print(f\"Hyperbolic: {np.mean(f1_scores_hrf):.3f} +/- {np.std(f1_scores_hrf):.3f}\")\n",
    "print(f\"Euclidean: {np.mean(f1_scores_rf):.3f} +/- {np.std(f1_scores_rf):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from HoroRF.datasets.gaussian import get_training_data, get_testing_data\n",
    "\n",
    "get_training_data(class_label=2, seed=0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([44, 44,  9, 22, 44, 44,  9, 44, 44,  9,  9, 22, 22, 44, 22, 44, 22, 22,\n",
       "        44, 22,  3, 22, 22, 44, 44, 44,  2,  3, 22, 22, 22,  2, 22,  2, 44, 44,\n",
       "        22,  9, 44, 44, 22, 44, 22, 22, 22,  9,  9, 44, 44, 22, 44,  3, 44, 22,\n",
       "         9, 43, 44, 22, 22, 44,  9,  3,  9,  2, 44, 43, 44, 22,  9,  3, 44,  9,\n",
       "        44, 44, 22,  9,  3,  3, 44,  2, 22,  2,  3, 22, 44,  3,  3,  3, 44, 44,\n",
       "        44,  2,  2, 22, 22, 22,  3,  2,  9, 22])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from HoroRF.datasets.neuroseed import get_training_data, get_testing_data\n",
    "\n",
    "get_training_data(class_label=2, seed=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['776992', '1050608', '190299', '358030', '239283', '4030157', '35786',\n",
       "       '174924', '370251', '191389',\n",
       "       ...\n",
       "       '268328', '228988', '155616', '158709', '299059', '515774', '311952',\n",
       "       '568082', '1112813', '562583'],\n",
       "      dtype='object', length=32863)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = adata.var[\"taxonomy_1\"]\n",
    "labels_counts = labels.value_counts()\n",
    "keep = labels_counts[labels_counts > 1000].index\n",
    "\n",
    "labels_filtered = labels[labels.isin(keep)]\n",
    "labels_filtered.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['145236', '4442899', '112801', '269532', '95741', '301910',\n",
       "       '74869', '1967053', '768535', '691952', '294040', '470879',\n",
       "       '358439', '95522', '268755', '4448558', '593016', '318205',\n",
       "       '4298060', '4475224', '1096766', '1108726', '3219862', '193763',\n",
       "       '2545365', '252198', '516020', '271500', '354401', '241499',\n",
       "       '4437436', '971971', '344456', '322087', '4371949', '554911',\n",
       "       '202816', '4444213', '4416974', '548878', '164915', '370295',\n",
       "       '4445508', '4321043', '4416763', '1087825', '997439', '4256699',\n",
       "       '3862524', '47181', '174004', '407459', '683241', '4364083',\n",
       "       '115049', '206331', '343699', '964799', '1667530', '4459355',\n",
       "       '583472', '4377731', '1105919', '814570', '709691', '145786',\n",
       "       '332210', '228043', '810672', '199344', '904468', '668257',\n",
       "       '4322804', '4320437', '4367317', '807112', '280233', '147940',\n",
       "       '1066654', '4469223', '563671', '2838675', '4468097', '4349553',\n",
       "       '1074801', '1117187', '998524', '4409486', '547904', '4438991',\n",
       "       '542729', '592901', '4478325', '1111892', '172081', '993372',\n",
       "       '510286', '726955', '755271', '2283850', '4455242', '4370941',\n",
       "       '301721', '175193', '613280', '3746876', '242138', '571103',\n",
       "       '300152', '4324420', '4418197', '4471635', '252702', '809489',\n",
       "       '561294', '814435', '4451215', '4349836', '642096', '270239',\n",
       "       '368338', '177555', '1108599', '655207', '868676'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "indices = np.random.choice(labels_filtered.index, 125, replace=False)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out error with my method:\n",
    "\n",
    "seed = 15\n",
    "dim = 4\n",
    "from HoroRF.datasets.neuroseed import get_training_data, get_testing_data\n",
    "from src.hyperdt.conversions import convert\n",
    "\n",
    "X, y = get_training_data(class_label=dim, seed=seed, num_samples=1000)\n",
    "X_h = convert(X.numpy(), \"poincare\", \"hyperboloid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:02<00:00,  9.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HyperbolicRandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HyperbolicRandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>HyperbolicRandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "HyperbolicRandomForestClassifier()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.hyperdt.forest import HyperbolicRandomForestClassifier\n",
    "\n",
    "hrf = HyperbolicRandomForestClassifier(n_estimators=24, max_depth=6)\n",
    "\n",
    "hrf.fit(X_h, y.numpy(), use_tqdm=True, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y.numpy(), hrf.predict(X_h), average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting poor neuroseed performance\n",
    "\n",
    "from HoroRF.datasets.neuroseed import get_training_data, get_testing_data\n",
    "from src.hyperdt.conversions import convert\n",
    "from src.hyperdt.tree import HyperbolicDecisionTreeClassifier\n",
    "\n",
    "X, y = get_training_data(class_label=2, seed=0, num_samples=400)\n",
    "X_h = convert(X.numpy(), \"poincare\", \"hyperboloid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 46.29it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (out=NoneType, axis=int, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py\", line 588, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py\", line 588, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/phil/hdt/src/hyperdt/tree.py\", line 239, in fit\n    self._validate_hyperbolic(X)\n  File \"/home/phil/hdt/src/hyperdt/tree.py\", line 207, in _validate_hyperbolic\n    np.sum(X_spacelike**2, axis=1) - X[:, self.timelike_dim] ** 2,\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 2313, in sum\n    return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 86, in _wrapreduction\n    return reduction(axis=axis, out=out, **passkwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: sum() received an invalid combination of arguments - got (out=NoneType, axis=int, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/phil/hdt/19_hororf.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m hrf \u001b[39m=\u001b[39m HyperbolicRandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_index, test_index \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     hrf\u001b[39m.\u001b[39mfit(X[train_index], y[train_index], use_tqdm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, seed\u001b[39m=\u001b[39mseed)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m hrf\u001b[39m.\u001b[39mpredict(X[test_index])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(f1_score(y[test_index], y_pred, average\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/hdt/src/hyperdt/forest.py:58\u001b[0m, in \u001b[0;36mRandomForestClassifier.fit\u001b[0;34m(self, X, y, use_tqdm, seed)\u001b[0m\n\u001b[1;32m     56\u001b[0m trees \u001b[39m=\u001b[39m tqdm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees) \u001b[39mif\u001b[39;00m use_tqdm \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m     fitted_trees \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)(\n\u001b[1;32m     59\u001b[0m         delayed(tree\u001b[39m.\u001b[39mfit)(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_subsample(X, y)) \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m trees\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees \u001b[39m=\u001b[39m fitted_trees\n\u001b[1;32m     62\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1939\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1942\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1944\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1586\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1587\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1691\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1685\u001b[0m \n\u001b[1;32m   1686\u001b[0m     \u001b[39m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[39m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[39m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m     \u001b[39m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aborting:\n\u001b[0;32m-> 1691\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1692\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m     \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m     \u001b[39m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1726\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[39m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[39m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[39m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[39mif\u001b[39;00m error_job \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1726\u001b[0m     error_job\u001b[39m.\u001b[39mget_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:735\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    729\u001b[0m backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_backend\n\u001b[1;32m    731\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    732\u001b[0m     \u001b[39m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[39m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[39m# be returned.\u001b[39;00m\n\u001b[0;32m--> 735\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_or_raise()\n\u001b[1;32m    737\u001b[0m \u001b[39m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:753\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    752\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 753\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[1;32m    754\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[1;32m    755\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (out=NoneType, axis=int, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "# So here's somewhere we fail:\n",
    "# n = 400, seed = 1, dim = 8\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.hyperdt.forest import HyperbolicRandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "n = 400\n",
    "seed = 1\n",
    "dim = 8\n",
    "\n",
    "X, y = get_training_data(class_label=dim, seed=seed, num_samples=n)\n",
    "X_h = convert(X.numpy(), \"poincare\", \"hyperboloid\")\n",
    "y = y.numpy()\n",
    "# Relabel\n",
    "_, y = np.unique(y, return_inverse=True)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "hrf = HyperbolicRandomForestClassifier(n_estimators=1, max_depth=6)\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "    hrf.fit(X[train_index], y[train_index], use_tqdm=True, seed=seed)\n",
    "    y_pred = hrf.predict(X[test_index])\n",
    "    print(f1_score(y[test_index], y_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf = HyperbolicRandomForestClassifier(n_estimators=1, max_depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrf.fit(X_h, y)\n",
    "\n",
    "f1_score(y, hrf.predict(X_h), average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  9, 22, 43, 44])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "reference = hrf.trees[0].classes_\n",
    "\n",
    "assert np.all(reference == hrf.classes_)\n",
    "assert np.all(reference == [tree.classes_ for tree in hrf.trees])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HyperbolicRandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HyperbolicRandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>HyperbolicRandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "HyperbolicRandomForestClassifier()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's step through each method, I guess\n",
    "\n",
    "from src.hyperdt.tree import HyperbolicDecisionTreeClassifier\n",
    "\n",
    "X, y = get_training_data(class_label=4, seed=15, num_samples=1000)\n",
    "X_h = convert(X.numpy(), \"poincare\", \"hyperboloid\")\n",
    "hdt = HyperbolicRandomForestClassifier(max_depth=6)\n",
    "hdt.fit(X_h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 9 is out of bounds for axis 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/phil/hdt/19_hororf.ipynb Cell 27\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m f1_score\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m y_pred \u001b[39m=\u001b[39m hdt\u001b[39m.\u001b[39mpredict(X_h)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m f1_score(y, hdt\u001b[39m.\u001b[39mclasses_[y_pred], average\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 9 is out of bounds for axis 0 with size 6"
     ]
    }
   ],
   "source": [
    "# First up, we see that f1 score is messed up:\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = hdt.predict(X_h)\n",
    "f1_score(y, hdt.classes_[y_pred], average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  9, 22, 43, 44])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdt.trees[0].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  9, 22, 43, 44])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdt.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  9, 22, 43, 44])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86875"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdt2 = HyperbolicDecisionTreeClassifier()\n",
    "hdt2.fit(X_h, y)\n",
    "f1_score(hdt2.predict(X_h), y, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02375"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrf2 = HyperbolicRandomForestClassifier()\n",
    "hrf2.fit(X_h, y)\n",
    "f1_score(hrf2.predict(X_h), y, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02375"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So retraining the tree gives us the same issue:\n",
    "hrf2.trees[0].fit(X_h, y)\n",
    "f1_score(hrf2.trees[0].predict(X_h), y, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04875, 0.06875, 0.16625, 0.3575 , 0.02625, 0.3325 ])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrf2.trees[0]._get_probs(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src.hyperdt.tree.DecisionTreeClassifier"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hrf2.trees[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Points must lie on a hyperboloid: Lorentzian Inner Product does not equal the curvature of 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/phil/hdt/src/hyperdt/tree.py\", line 206, in _validate_hyperbolic\n    assert np.allclose(\nAssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py\", line 588, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py\", line 588, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/phil/hdt/src/hyperdt/tree.py\", line 239, in fit\n    self._validate_hyperbolic(X)\n  File \"/home/phil/hdt/src/hyperdt/tree.py\", line 212, in _validate_hyperbolic\n    raise ValueError(\"Points must lie on a hyperboloid: Lorentzian Inner Product does not equal the curvature of {}.\".format(self.curvature))\nValueError: Points must lie on a hyperboloid: Lorentzian Inner Product does not equal the curvature of 1.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/phil/hdt/19_hororf.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhyperdt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mforest\u001b[39;00m \u001b[39mimport\u001b[39;00m HyperbolicRandomForestClassifier\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m hrf \u001b[39m=\u001b[39m HyperbolicRandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m hrf\u001b[39m.\u001b[39mfit(X, y)\n",
      "File \u001b[0;32m~/hdt/src/hyperdt/forest.py:58\u001b[0m, in \u001b[0;36mRandomForestClassifier.fit\u001b[0;34m(self, X, y, use_tqdm, seed)\u001b[0m\n\u001b[1;32m     56\u001b[0m trees \u001b[39m=\u001b[39m tqdm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees) \u001b[39mif\u001b[39;00m use_tqdm \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m     fitted_trees \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)(\n\u001b[1;32m     59\u001b[0m         delayed(tree\u001b[39m.\u001b[39mfit)(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_subsample(X, y)) \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m trees\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees \u001b[39m=\u001b[39m fitted_trees\n\u001b[1;32m     62\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1939\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1942\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1944\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1586\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1587\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1691\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1685\u001b[0m \n\u001b[1;32m   1686\u001b[0m     \u001b[39m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[39m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[39m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m     \u001b[39m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aborting:\n\u001b[0;32m-> 1691\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1692\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m     \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m     \u001b[39m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1726\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[39m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[39m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[39m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[39mif\u001b[39;00m error_job \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1726\u001b[0m     error_job\u001b[39m.\u001b[39mget_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:735\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    729\u001b[0m backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_backend\n\u001b[1;32m    731\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    732\u001b[0m     \u001b[39m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[39m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[39m# be returned.\u001b[39;00m\n\u001b[0;32m--> 735\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_or_raise()\n\u001b[1;32m    737\u001b[0m \u001b[39m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:753\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    752\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 753\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[1;32m    754\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[1;32m    755\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Points must lie on a hyperboloid: Lorentzian Inner Product does not equal the curvature of 1."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 0\n",
    "dim = 8\n",
    "\n",
    "from HoroRF.datasets.gaussian import get_training_data, get_testing_data\n",
    "\n",
    "X, y = get_training_data(class_label=dim, seed=seed, num_samples=1000, convert=False)\n",
    "X = X.numpy()\n",
    "y = y.numpy()\n",
    "# X_h = convert(X.numpy(), \"poincare\", \"hyperboloid\")\n",
    "\n",
    "from src.hyperdt.forest import HyperbolicRandomForestClassifier\n",
    "\n",
    "hrf = HyperbolicRandomForestClassifier(n_estimators=1, max_depth=6)\n",
    "hrf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -1.015625  , -1.        , -1.        , -0.99999952,\n",
       "       -1.        , -1.        , -1.00000095, -1.        , -1.        ,\n",
       "       -1.00000381, -1.00000012, -1.        , -1.        , -1.        ,\n",
       "       -1.00000006, -1.        , -1.00000763, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -0.99999237, -1.        ,\n",
       "       -0.99999619, -1.        , -1.        , -1.015625  , -1.        ,\n",
       "       -0.99999976, -0.99999999, -1.        , -1.        , -1.00001526,\n",
       "       -0.99999997, -1.        , -1.        , -1.00000012, -1.        ,\n",
       "       -1.00000048, -1.        , -1.        , -1.00000381, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00000191, -1.        ,\n",
       "       -1.00000012, -0.99999999, -1.        , -1.        , -0.99999999,\n",
       "       -1.        , -1.        , -1.        , -1.00000048, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -0.99999999, -0.9921875 , -1.00000381,\n",
       "       -1.00000191, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -0.99902344,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.00000003, -0.99999952, -1.        ,\n",
       "       -1.        , -1.        , -0.99999997, -1.        , -1.00048828,\n",
       "       -1.        , -1.        , -1.        , -1.00000006, -0.99804688,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.99999952, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.00000001, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00000001, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -0.99999952, -1.        ,\n",
       "       -1.        , -1.        , -1.00000001, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -0.99975586,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.99999999, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.00000002, -1.        , -0.99999905, -1.        ,\n",
       "       -0.99999237, -1.        , -1.        , -0.99999999, -1.        ,\n",
       "       -1.        , -1.00000006, -1.        , -1.00195312, -1.        ,\n",
       "       -1.        , -1.5       , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -0.99999237, -1.00006104, -1.        , -0.99999999,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.00000191, -1.00000048, -1.        , -1.        ,\n",
       "       -0.99999905, -1.00000095, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.00000006, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -0.99999999,\n",
       "       -1.00000012, -1.        , -1.00000003, -1.        , -1.00000048,\n",
       "       -1.        , -1.        , -0.99999997, -1.00000001, -1.        ,\n",
       "       -1.00000024, -1.00000001, -0.99999619, -1.        , -1.        ,\n",
       "       -1.        , -0.99987793, -1.        , -1.        , -1.        ,\n",
       "       -0.99999988, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.00000003, -1.        , -1.        ,\n",
       "       -0.99609375, -1.        , -1.        , -0.99999619, -0.99999619,\n",
       "       -1.00000048, -1.        , -1.        , -1.        , -1.00000001,\n",
       "       -1.        , -1.        , -1.        , -1.00000095, -0.99999809,\n",
       "       -1.        , -1.        , -1.00000001, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00000012, -0.99998474,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.99999999, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.99999994, -1.00000012, -1.        , -1.        , -1.        ,\n",
       "       -0.99999999, -1.        , -1.00000012, -1.        , -0.99999988,\n",
       "       -0.99975586, -1.        , -1.        , -0.99999994, -0.99999809,\n",
       "       -1.        , -0.99999994, -1.00000024, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00000381, -0.99999999,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.00097656,\n",
       "       -1.        , -1.        , -1.        , -0.99975586, -1.        ,\n",
       "       -0.99999988, -1.        , -0.99999999, -0.99999619, -1.00000018,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -0.984375  , -1.        , -0.99999976, -1.        ,\n",
       "       -1.        , -1.00048828, -0.99999997, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -0.99999988, -1.        ,\n",
       "       -1.        , -1.00000006, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -0.99999999, -1.        , -1.00000191, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -0.99999619, -1.        , -1.        , -1.00000006,\n",
       "       -1.00000024, -1.        , -1.        , -1.00000095, -1.        ,\n",
       "       -1.00000001, -1.0078125 , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00000001, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00000003, -0.99987793,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.00000012, -0.99999999, -1.        , -1.00001526, -0.99993896,\n",
       "       -1.00001526, -0.99999952, -1.        , -1.        , -0.99999994,\n",
       "       -1.        , -1.        , -1.        , -0.99999976, -0.99999999,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.99987793, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.00000006, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -0.99996948, -1.        ,\n",
       "       -0.99999999, -1.00000003, -1.        , -1.        , -0.99902344,\n",
       "       -1.0078125 , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.00000001,\n",
       "       -0.99999619,  0.        , -0.99999905, -0.99999999, -1.00000001,\n",
       "       -1.00000001, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -0.99999999, -1.00000191, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00000001, -1.        ,\n",
       "       -1.        , -1.00000381, -1.        , -1.        , -1.00000003,\n",
       "       -0.99999988, -1.00000048, -0.99999988, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -0.99999988, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.00000012, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.99951172, -0.99999237, -1.        , -1.        , -0.99999988,\n",
       "       -1.        , -0.99999619, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.00195312, -1.        , -0.99999999,\n",
       "       -1.        , -1.        , -1.00000001, -1.        , -1.        ,\n",
       "       -1.00003052, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.015625  , -1.        , -1.        , -1.        , -0.99998474,\n",
       "       -0.99999976, -1.        , -0.99999982, -1.00000006, -1.        ,\n",
       "       -1.        , -0.99975586, -1.00000006, -1.        , -1.        ,\n",
       "       -1.        , -1.00000003, -1.        , -1.        , -1.        ,\n",
       "       -1.00000001, -0.99999619, -1.        , -1.        , -1.        ,\n",
       "       -1.00000006, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00000191, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.015625  , -0.99999999, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -0.99999997, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00003052, -1.        ,\n",
       "       -0.99999857, -1.        , -1.        , -1.00000024, -1.00000024,\n",
       "       -1.        , -0.99999976, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.00000006, -1.        , -1.        , -1.25      ,\n",
       "       -1.        , -1.        , -1.00000072, -1.        , -0.99999997,\n",
       "       -1.        , -1.00000024, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.00000381,\n",
       "       -1.        , -1.        , -1.00000095, -1.        , -1.00000001,\n",
       "       -1.00000095, -1.        , -0.99902344, -1.00006104, -1.        ,\n",
       "       -1.00000001, -0.99996948, -1.00012207, -0.99999997, -0.99999905,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.00000003, -0.99902344, -1.        ,\n",
       "       -1.        , -1.00000006, -1.        , -0.99999999, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -0.99951172, -1.00000006,\n",
       "       -0.99999999, -0.99951172, -1.        , -1.        , -1.00000006,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.00000572,\n",
       "       -1.        , -1.        , -0.99999999, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -0.99999997, -1.        , -1.        ,\n",
       "       -1.        , -0.99999988, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00000006, -0.99999998,\n",
       "        8.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.00000003, -1.        , -0.99999809, -0.99993896, -1.00000024,\n",
       "       -1.        , -1.        , -1.00000001, -1.        , -1.        ,\n",
       "       -1.        , -0.99999976, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00003052, -1.00000003,\n",
       "       -1.        , -1.        , -1.00000003, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.00000001, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.99999999, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -0.99999997, -1.        , -1.        , -1.00000095, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -0.99999988, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.00000006, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.00024414, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.00000003, -1.        , -1.        ,\n",
       "       -0.99999237, -1.        , -1.        , -1.00000024, -1.        ,\n",
       "       -1.00000024, -1.        , -0.99999994, -1.        , -1.00001526,\n",
       "       -1.        , -1.        , -1.00000191, -1.        , -1.        ,\n",
       "       -0.99975586, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.00024414, -1.00000095, -1.        , -1.        , -1.        ,\n",
       "       -1.00000095, -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.00000191, -1.        , -1.        , -1.00024414, -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.00000381, -1.        ,\n",
       "       -0.99999997, -1.        , -1.00000003, -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X[:, 1:] ** 2, axis=1) - X[:, 0] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0345e+01,  3.9903e+00,  9.4921e+00],\n",
       "        [ 4.8074e+00,  4.5363e+00,  1.2381e+00],\n",
       "        [ 1.1840e+01,  4.7350e+00,  1.0806e+01],\n",
       "        [ 3.0232e+00,  2.8361e+00,  3.1029e-01],\n",
       "        [ 5.6779e+00,  2.3141e+00,  5.0876e+00],\n",
       "        [ 1.0710e+01,  1.0251e+01,  2.9358e+00],\n",
       "        [ 3.4341e+01,  3.3381e+01,  8.0009e+00],\n",
       "        [ 2.2317e+00,  1.9815e+00,  2.3266e-01],\n",
       "        [ 7.0170e+00,  2.8241e+00,  6.3453e+00],\n",
       "        [ 4.8746e+00,  1.8847e+00,  4.3828e+00],\n",
       "        [ 3.5791e+00,  3.3669e+00,  6.8831e-01],\n",
       "        [ 1.0764e+01,  4.1736e+00,  9.8718e+00],\n",
       "        [ 1.7322e+01,  1.6808e+01,  4.0674e+00],\n",
       "        [ 1.5636e+01,  6.0609e+00,  1.4379e+01],\n",
       "        [ 1.2036e+01,  4.5829e+00,  1.1085e+01],\n",
       "        [ 5.3760e+00,  5.0937e+00,  1.3984e+00],\n",
       "        [ 1.9776e+00,  1.7060e+00,  2.3058e-02],\n",
       "        [ 1.6574e+01,  6.5419e+00,  1.5195e+01],\n",
       "        [ 1.4714e+00,  1.0318e+00,  3.1724e-01],\n",
       "        [ 2.8383e+00,  1.2815e+00,  2.3268e+00],\n",
       "        [ 1.7756e+01,  1.7079e+01,  4.7536e+00],\n",
       "        [ 1.9374e+00,  1.1081e+00,  1.2352e+00],\n",
       "        [ 1.4975e+01,  6.0043e+00,  1.3683e+01],\n",
       "        [ 5.9165e+00,  2.3225e+00,  5.3489e+00],\n",
       "        [ 1.7718e+00,  9.2508e-01,  1.1330e+00],\n",
       "        [ 3.8031e+00,  1.5740e+00,  3.3145e+00],\n",
       "        [ 1.1495e+01,  1.1175e+01,  2.4996e+00],\n",
       "        [ 4.2996e+00,  1.7157e+00,  3.8135e+00],\n",
       "        [ 1.2448e+01,  4.8664e+00,  1.1414e+01],\n",
       "        [ 3.1093e+01,  1.2164e+01,  2.8597e+01],\n",
       "        [ 1.5378e+01,  1.4995e+01,  3.2647e+00],\n",
       "        [ 1.7131e+00,  1.3290e+00, -4.1047e-01],\n",
       "        [ 3.8610e+00,  1.5272e+00,  3.4022e+00],\n",
       "        [ 1.1167e+02,  1.0811e+02,  2.7941e+01],\n",
       "        [ 4.5839e+00,  1.7831e+00,  4.1028e+00],\n",
       "        [ 1.7365e+00,  5.6969e-01,  1.3004e+00],\n",
       "        [ 1.3957e+00,  6.2789e-01,  7.4420e-01],\n",
       "        [ 8.6508e+00,  3.3207e+00,  7.9252e+00],\n",
       "        [ 5.2952e+00,  5.0603e+00,  1.1966e+00],\n",
       "        [ 3.9049e+00,  1.6308e+00,  3.4042e+00],\n",
       "        [ 6.0912e+00,  2.3941e+00,  5.5110e+00],\n",
       "        [ 1.8292e+00,  1.4682e+00,  4.3607e-01],\n",
       "        [ 5.7770e+01,  5.6074e+01,  1.3858e+01],\n",
       "        [ 3.1014e+00,  1.2732e+00,  2.6453e+00],\n",
       "        [ 1.1920e+00,  5.5740e-01, -3.3176e-01],\n",
       "        [ 6.7455e+01,  6.5474e+01,  1.6198e+01],\n",
       "        [ 3.6948e+00,  1.3852e+00,  3.2760e+00],\n",
       "        [ 2.6947e+01,  2.5913e+01,  7.3228e+00],\n",
       "        [ 5.3238e+01,  5.1558e+01,  1.3231e+01],\n",
       "        [ 4.7868e+00,  1.9431e+00,  4.2588e+00],\n",
       "        [ 7.2200e+00,  2.9599e+00,  6.5090e+00],\n",
       "        [ 5.1714e+00,  2.0228e+00,  4.6531e+00],\n",
       "        [ 3.3772e+00,  1.5027e+00,  2.8543e+00],\n",
       "        [ 5.6755e+00,  2.3651e+00,  5.0614e+00],\n",
       "        [ 1.1434e+01,  4.4762e+00,  1.0474e+01],\n",
       "        [ 3.8546e+00,  1.6462e+00,  3.3389e+00],\n",
       "        [ 1.3367e+01,  1.2843e+01,  3.5689e+00],\n",
       "        [ 4.2915e+00,  1.7054e+00,  3.8091e+00],\n",
       "        [ 4.7037e+00,  1.8306e+00,  4.2159e+00],\n",
       "        [ 1.2388e+00,  6.8670e-01, -2.5093e-01],\n",
       "        [ 1.0795e+01,  4.2660e+00,  9.8661e+00],\n",
       "        [ 5.6632e+00,  5.3729e+00,  1.4845e+00],\n",
       "        [ 1.7307e+00,  1.4025e+00,  1.6846e-01],\n",
       "        [ 5.1950e+00,  1.9736e+00,  4.7003e+00],\n",
       "        [ 7.2827e+00,  2.9725e+00,  6.5728e+00],\n",
       "        [ 5.9224e+00,  2.4805e+00,  5.2841e+00],\n",
       "        [ 1.9646e+00,  1.3235e+00, -1.0526e+00],\n",
       "        [ 1.9941e+01,  7.8129e+00,  1.8320e+01],\n",
       "        [ 1.9755e+01,  1.9015e+01,  5.2625e+00],\n",
       "        [ 3.8013e+01,  3.6784e+01,  9.5372e+00],\n",
       "        [ 2.5238e+00,  1.1118e+00,  2.0330e+00],\n",
       "        [ 2.0561e+00,  1.1332e+00,  1.3941e+00],\n",
       "        [ 5.1245e+00,  1.9955e+00,  4.6129e+00],\n",
       "        [ 3.1298e+00,  1.3913e+00,  2.6191e+00],\n",
       "        [ 5.6446e+00,  2.3213e+00,  5.0471e+00],\n",
       "        [ 5.0384e+00,  4.8384e+00,  9.8783e-01],\n",
       "        [ 2.0684e+01,  7.9613e+00,  1.9064e+01],\n",
       "        [ 2.2680e+00,  1.9763e+00,  4.8790e-01],\n",
       "        [ 1.3836e+00,  7.1082e-01, -6.3955e-01],\n",
       "        [ 1.8996e+00,  9.3536e-01, -1.3167e+00],\n",
       "        [ 1.3754e+01,  5.4344e+00,  1.2596e+01],\n",
       "        [ 1.3939e+01,  1.3391e+01,  3.7382e+00],\n",
       "        [ 7.6649e+00,  3.0205e+00,  6.9733e+00],\n",
       "        [ 3.0155e+00,  2.8230e+00,  3.5203e-01],\n",
       "        [ 4.5803e+00,  1.6311e+00,  4.1615e+00],\n",
       "        [ 2.2913e+01,  8.8346e+00,  2.1118e+01],\n",
       "        [ 2.1030e+00,  8.3394e-01,  1.6514e+00],\n",
       "        [ 1.3554e+00,  4.8615e-01, -7.7515e-01],\n",
       "        [ 2.0672e+00,  8.7389e-01,  1.5842e+00],\n",
       "        [ 2.0948e+00,  9.7324e-01,  1.5624e+00],\n",
       "        [ 4.0080e+01,  1.5688e+01,  3.6869e+01],\n",
       "        [ 1.1696e+00,  4.2548e-01, -4.3236e-01],\n",
       "        [ 4.7944e+00,  1.9414e+00,  4.2682e+00],\n",
       "        [ 1.0685e+01,  4.2082e+00,  9.7699e+00],\n",
       "        [ 1.3892e+01,  5.3345e+00,  1.2788e+01],\n",
       "        [ 1.3758e+00,  8.6131e-01,  3.8847e-01],\n",
       "        [ 1.1298e+00,  2.5246e-01, -4.6124e-01],\n",
       "        [ 5.4981e+00,  2.1808e+00,  4.9471e+00],\n",
       "        [ 9.4248e+00,  9.0190e+00,  2.5466e+00],\n",
       "        [ 3.5001e+00,  1.4062e+00,  3.0452e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from HoroRF.datasets.gaussian import get_training_data, get_testing_data\n",
    "\n",
    "X, y = get_training_data(class_label=2, seed=0, num_samples=125, convert_to_poincare=False)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1298, dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AxisArrays with keys: component_embeddings_euclidean_128, component_embeddings_euclidean_16, component_embeddings_euclidean_2, component_embeddings_euclidean_32, component_embeddings_euclidean_4, component_embeddings_euclidean_64, component_embeddings_euclidean_8, component_embeddings_hyperboloid_128, component_embeddings_hyperboloid_16, component_embeddings_hyperboloid_2, component_embeddings_hyperboloid_32, component_embeddings_hyperboloid_4, component_embeddings_hyperboloid_64, component_embeddings_hyperboloid_8, component_embeddings_poincare_128, component_embeddings_poincare_16, component_embeddings_poincare_2, component_embeddings_poincare_32, component_embeddings_poincare_4, component_embeddings_poincare_64, component_embeddings_poincare_8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = anndata.read_h5ad(\"/home/phil/americangut/data/big_table_with_embeddings.h5ad\")\n",
    "\n",
    "adata.varm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>776992</th>\n",
       "      <td>0.055982</td>\n",
       "      <td>-0.048648</td>\n",
       "      <td>0.016134</td>\n",
       "      <td>0.119837</td>\n",
       "      <td>-0.538352</td>\n",
       "      <td>-0.155689</td>\n",
       "      <td>-0.665964</td>\n",
       "      <td>-0.037884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050608</th>\n",
       "      <td>-0.113340</td>\n",
       "      <td>-0.229799</td>\n",
       "      <td>0.133146</td>\n",
       "      <td>0.012594</td>\n",
       "      <td>-0.227479</td>\n",
       "      <td>-0.169610</td>\n",
       "      <td>-0.767100</td>\n",
       "      <td>-0.163189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190299</th>\n",
       "      <td>0.014210</td>\n",
       "      <td>-0.004440</td>\n",
       "      <td>-0.026317</td>\n",
       "      <td>0.363405</td>\n",
       "      <td>-0.272027</td>\n",
       "      <td>-0.070862</td>\n",
       "      <td>-0.652109</td>\n",
       "      <td>-0.376673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358030</th>\n",
       "      <td>0.101718</td>\n",
       "      <td>-0.107645</td>\n",
       "      <td>-0.035459</td>\n",
       "      <td>0.213485</td>\n",
       "      <td>-0.343706</td>\n",
       "      <td>0.013884</td>\n",
       "      <td>-0.741350</td>\n",
       "      <td>-0.205995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239283</th>\n",
       "      <td>-0.455948</td>\n",
       "      <td>-0.474233</td>\n",
       "      <td>-0.215332</td>\n",
       "      <td>0.207518</td>\n",
       "      <td>-0.190398</td>\n",
       "      <td>-0.150162</td>\n",
       "      <td>-0.257874</td>\n",
       "      <td>-0.362781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105573</th>\n",
       "      <td>0.231628</td>\n",
       "      <td>-0.376365</td>\n",
       "      <td>0.233156</td>\n",
       "      <td>-0.044553</td>\n",
       "      <td>-0.380642</td>\n",
       "      <td>0.012487</td>\n",
       "      <td>-0.228628</td>\n",
       "      <td>-0.574599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311952</th>\n",
       "      <td>0.181175</td>\n",
       "      <td>-0.108128</td>\n",
       "      <td>-0.049317</td>\n",
       "      <td>0.300540</td>\n",
       "      <td>-0.263796</td>\n",
       "      <td>-0.065274</td>\n",
       "      <td>-0.683462</td>\n",
       "      <td>-0.317625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568082</th>\n",
       "      <td>-0.384161</td>\n",
       "      <td>-0.525431</td>\n",
       "      <td>-0.174787</td>\n",
       "      <td>0.065428</td>\n",
       "      <td>-0.226401</td>\n",
       "      <td>-0.251183</td>\n",
       "      <td>-0.290468</td>\n",
       "      <td>-0.349166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112813</th>\n",
       "      <td>-0.041194</td>\n",
       "      <td>-0.109654</td>\n",
       "      <td>0.138560</td>\n",
       "      <td>0.091734</td>\n",
       "      <td>-0.755047</td>\n",
       "      <td>-0.224828</td>\n",
       "      <td>-0.274242</td>\n",
       "      <td>-0.204795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562583</th>\n",
       "      <td>0.087747</td>\n",
       "      <td>-0.173656</td>\n",
       "      <td>0.023036</td>\n",
       "      <td>0.280883</td>\n",
       "      <td>-0.449570</td>\n",
       "      <td>0.100550</td>\n",
       "      <td>-0.336315</td>\n",
       "      <td>-0.580098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37215 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "776992   0.055982 -0.048648  0.016134  0.119837 -0.538352 -0.155689 -0.665964   \n",
       "1050608 -0.113340 -0.229799  0.133146  0.012594 -0.227479 -0.169610 -0.767100   \n",
       "190299   0.014210 -0.004440 -0.026317  0.363405 -0.272027 -0.070862 -0.652109   \n",
       "358030   0.101718 -0.107645 -0.035459  0.213485 -0.343706  0.013884 -0.741350   \n",
       "239283  -0.455948 -0.474233 -0.215332  0.207518 -0.190398 -0.150162 -0.257874   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1105573  0.231628 -0.376365  0.233156 -0.044553 -0.380642  0.012487 -0.228628   \n",
       "311952   0.181175 -0.108128 -0.049317  0.300540 -0.263796 -0.065274 -0.683462   \n",
       "568082  -0.384161 -0.525431 -0.174787  0.065428 -0.226401 -0.251183 -0.290468   \n",
       "1112813 -0.041194 -0.109654  0.138560  0.091734 -0.755047 -0.224828 -0.274242   \n",
       "562583   0.087747 -0.173656  0.023036  0.280883 -0.449570  0.100550 -0.336315   \n",
       "\n",
       "                7  \n",
       "776992  -0.037884  \n",
       "1050608 -0.163189  \n",
       "190299  -0.376673  \n",
       "358030  -0.205995  \n",
       "239283  -0.362781  \n",
       "...           ...  \n",
       "1105573 -0.574599  \n",
       "311952  -0.317625  \n",
       "568082  -0.349166  \n",
       "1112813 -0.204795  \n",
       "562583  -0.580098  \n",
       "\n",
       "[37215 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.varm[\"component_embeddings_poincare_8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component_embeddings_hyperboloid_128 Success\n",
      "component_embeddings_hyperboloid_16 Success\n",
      "component_embeddings_hyperboloid_2 Success\n",
      "component_embeddings_hyperboloid_32 Success\n",
      "component_embeddings_hyperboloid_4 Success\n",
      "component_embeddings_hyperboloid_64 Success\n",
      "component_embeddings_hyperboloid_8 Success\n"
     ]
    }
   ],
   "source": [
    "# Update adata to only have pandas dataframes\n",
    "import anndata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "adata = anndata.read_h5ad(\"/home/phil/americangut/data/big_table_with_embeddings.h5ad\")\n",
    "for key in adata.varm.keys():\n",
    "    if isinstance(adata.varm[key], pd.DataFrame):\n",
    "        continue\n",
    "    data = adata.varm[key]\n",
    "    # Put last column first\n",
    "    data = np.concatenate([data[:, -1:], data[:, :-1]], axis=1)\n",
    "    adata.varm[key] = pd.DataFrame(\n",
    "        data, index=adata.var_names, columns=[str(x) for x in range(adata.varm[key].shape[1])]\n",
    "    )\n",
    "    adata.varm[key].columns = [str(x) for x in adata.varm[key].columns]\n",
    "    print(key, \"Success\")\n",
    "\n",
    "# Save back in old location \"fixed\"\n",
    "adata.write_h5ad(\"/home/phil/americangut/data/big_table_with_embeddings_fixed.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>776992</th>\n",
       "      <td>0.003104</td>\n",
       "      <td>-0.011133</td>\n",
       "      <td>-0.050578</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>-0.012173</td>\n",
       "      <td>0.016810</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>-0.037227</td>\n",
       "      <td>0.026162</td>\n",
       "      <td>-0.002505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004643</td>\n",
       "      <td>-0.005937</td>\n",
       "      <td>0.220302</td>\n",
       "      <td>0.013990</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>-0.004111</td>\n",
       "      <td>0.009860</td>\n",
       "      <td>-0.025747</td>\n",
       "      <td>1.028866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050608</th>\n",
       "      <td>0.014252</td>\n",
       "      <td>0.035550</td>\n",
       "      <td>-0.035257</td>\n",
       "      <td>-0.001957</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.026523</td>\n",
       "      <td>0.006831</td>\n",
       "      <td>0.017551</td>\n",
       "      <td>0.007044</td>\n",
       "      <td>0.038042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>-0.007701</td>\n",
       "      <td>0.219405</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>-0.017631</td>\n",
       "      <td>-0.013085</td>\n",
       "      <td>-0.012843</td>\n",
       "      <td>-0.021901</td>\n",
       "      <td>-0.006877</td>\n",
       "      <td>1.028866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190299</th>\n",
       "      <td>-0.008894</td>\n",
       "      <td>0.007607</td>\n",
       "      <td>-0.031327</td>\n",
       "      <td>0.009640</td>\n",
       "      <td>0.028208</td>\n",
       "      <td>-0.047692</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.021859</td>\n",
       "      <td>-0.016243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013791</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>0.219097</td>\n",
       "      <td>-0.008274</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.034112</td>\n",
       "      <td>-0.025770</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>1.028866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358030</th>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>-0.044778</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>-0.028064</td>\n",
       "      <td>0.006646</td>\n",
       "      <td>-0.016058</td>\n",
       "      <td>-0.011688</td>\n",
       "      <td>-0.003859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>0.015618</td>\n",
       "      <td>0.223476</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>-0.009921</td>\n",
       "      <td>0.006784</td>\n",
       "      <td>-0.027722</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>-0.012998</td>\n",
       "      <td>1.028866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239283</th>\n",
       "      <td>0.011081</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>0.054537</td>\n",
       "      <td>-0.040107</td>\n",
       "      <td>-0.005428</td>\n",
       "      <td>0.049692</td>\n",
       "      <td>-0.038423</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>-0.024795</td>\n",
       "      <td>-0.041465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>-0.015846</td>\n",
       "      <td>0.192871</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-0.016512</td>\n",
       "      <td>-0.007969</td>\n",
       "      <td>-0.037395</td>\n",
       "      <td>-0.025141</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>1.028866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105573</th>\n",
       "      <td>0.026905</td>\n",
       "      <td>0.045144</td>\n",
       "      <td>0.073505</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>-0.004424</td>\n",
       "      <td>0.016115</td>\n",
       "      <td>-0.025248</td>\n",
       "      <td>-0.040511</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>0.013144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026041</td>\n",
       "      <td>-0.021510</td>\n",
       "      <td>0.198608</td>\n",
       "      <td>-0.007999</td>\n",
       "      <td>-0.019387</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>-0.001558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>-0.001082</td>\n",
       "      <td>1.028866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311952</th>\n",
       "      <td>-0.010087</td>\n",
       "      <td>0.021303</td>\n",
       "      <td>-0.026052</td>\n",
       "      <td>0.008121</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>-0.025448</td>\n",
       "      <td>0.027076</td>\n",
       "      <td>-0.009810</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>-0.021746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048315</td>\n",
       "      <td>-0.015067</td>\n",
       "      <td>0.222175</td>\n",
       "      <td>-0.005761</td>\n",
       "      <td>-0.012113</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>-0.013163</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>-0.008268</td>\n",
       "      <td>1.028866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568082</th>\n",
       "      <td>-0.005790</td>\n",
       "      <td>-0.025528</td>\n",
       "      <td>0.051072</td>\n",
       "      <td>-0.033486</td>\n",
       "      <td>0.024429</td>\n",
       "      <td>0.055403</td>\n",
       "      <td>-0.037198</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>-0.015689</td>\n",
       "      <td>-0.043806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007930</td>\n",
       "      <td>-0.007972</td>\n",
       "      <td>0.187760</td>\n",
       "      <td>0.025688</td>\n",
       "      <td>-0.024158</td>\n",
       "      <td>-0.014025</td>\n",
       "      <td>-0.040272</td>\n",
       "      <td>-0.011225</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>1.028866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112813</th>\n",
       "      <td>-0.007514</td>\n",
       "      <td>-0.047063</td>\n",
       "      <td>0.028568</td>\n",
       "      <td>-0.004904</td>\n",
       "      <td>-0.012186</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>-0.013998</td>\n",
       "      <td>-0.006658</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012517</td>\n",
       "      <td>0.030499</td>\n",
       "      <td>0.206941</td>\n",
       "      <td>0.010880</td>\n",
       "      <td>0.052308</td>\n",
       "      <td>-0.013916</td>\n",
       "      <td>0.009443</td>\n",
       "      <td>0.010271</td>\n",
       "      <td>-0.017667</td>\n",
       "      <td>1.028866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562583</th>\n",
       "      <td>0.020958</td>\n",
       "      <td>-0.004201</td>\n",
       "      <td>0.055232</td>\n",
       "      <td>0.010492</td>\n",
       "      <td>0.008546</td>\n",
       "      <td>-0.027887</td>\n",
       "      <td>-0.013806</td>\n",
       "      <td>-0.019488</td>\n",
       "      <td>-0.000245</td>\n",
       "      <td>-0.012909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>0.213029</td>\n",
       "      <td>-0.021138</td>\n",
       "      <td>-0.011956</td>\n",
       "      <td>-0.012456</td>\n",
       "      <td>-0.002847</td>\n",
       "      <td>-0.027528</td>\n",
       "      <td>-0.037615</td>\n",
       "      <td>1.028866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37215 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6   \\\n",
       "776992   0.003104 -0.011133 -0.050578  0.002356 -0.012173  0.016810  0.007254   \n",
       "1050608  0.014252  0.035550 -0.035257 -0.001957  0.000065  0.026523  0.006831   \n",
       "190299  -0.008894  0.007607 -0.031327  0.009640  0.028208 -0.047692  0.005988   \n",
       "358030   0.002423  0.016628 -0.044778  0.008861  0.008701 -0.028064  0.006646   \n",
       "239283   0.011081 -0.026398  0.054537 -0.040107 -0.005428  0.049692 -0.038423   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1105573  0.026905  0.045144  0.073505 -0.000142 -0.004424  0.016115 -0.025248   \n",
       "311952  -0.010087  0.021303 -0.026052  0.008121  0.010004 -0.025448  0.027076   \n",
       "568082  -0.005790 -0.025528  0.051072 -0.033486  0.024429  0.055403 -0.037198   \n",
       "1112813 -0.007514 -0.047063  0.028568 -0.004904 -0.012186  0.021110 -0.013998   \n",
       "562583   0.020958 -0.004201  0.055232  0.010492  0.008546 -0.027887 -0.013806   \n",
       "\n",
       "               7         8         9   ...        23        24        25  \\\n",
       "776992  -0.037227  0.026162 -0.002505  ... -0.004643 -0.005937  0.220302   \n",
       "1050608  0.017551  0.007044  0.038042  ...  0.008326 -0.007701  0.219405   \n",
       "190299   0.000386  0.021859 -0.016243  ...  0.013791  0.009728  0.219097   \n",
       "358030  -0.016058 -0.011688 -0.003859  ...  0.015823  0.015618  0.223476   \n",
       "239283   0.003105 -0.024795 -0.041465  ...  0.010443 -0.015846  0.192871   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "1105573 -0.040511  0.003926  0.013144  ...  0.026041 -0.021510  0.198608   \n",
       "311952  -0.009810  0.006768 -0.021746  ...  0.048315 -0.015067  0.222175   \n",
       "568082   0.017570 -0.015689 -0.043806  ...  0.007930 -0.007972  0.187760   \n",
       "1112813 -0.006658  0.007722 -0.002570  ...  0.012517  0.030499  0.206941   \n",
       "562583  -0.019488 -0.000245 -0.012909  ...  0.031455  0.004776  0.213029   \n",
       "\n",
       "               26        27        28        29        30        31        32  \n",
       "776992   0.013990  0.022461  0.001150 -0.004111  0.009860 -0.025747  1.028866  \n",
       "1050608  0.000881 -0.017631 -0.013085 -0.012843 -0.021901 -0.006877  1.028866  \n",
       "190299  -0.008274 -0.026201 -0.034112 -0.025770  0.015903  0.002340  1.028866  \n",
       "358030   0.002004 -0.009921  0.006784 -0.027722  0.005263 -0.012998  1.028866  \n",
       "239283   0.000068 -0.016512 -0.007969 -0.037395 -0.025141  0.007671  1.028866  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "1105573 -0.007999 -0.019387  0.016371 -0.001558 -0.021053 -0.001082  1.028866  \n",
       "311952  -0.005761 -0.012113  0.003765 -0.013163  0.009541 -0.008268  1.028866  \n",
       "568082   0.025688 -0.024158 -0.014025 -0.040272 -0.011225  0.003613  1.028866  \n",
       "1112813  0.010880  0.052308 -0.013916  0.009443  0.010271 -0.017667  1.028866  \n",
       "562583  -0.021138 -0.011956 -0.012456 -0.002847 -0.027528 -0.037615  1.028866  \n",
       "\n",
       "[37215 rows x 33 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(adata.varm[\"component_embeddings_hyperboloid_32\"], index=adata.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxonomy_1\n",
       "p__Firmicutes        11533\n",
       "p__Proteobacteria    10620\n",
       "p__Bacteroidetes      4990\n",
       "p__Actinobacteria     2977\n",
       "p__Acidobacteria      1640\n",
       "p__Planctomycetes     1103\n",
       "p__Cyanobacteria       763\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.var[\"taxonomy_1\"].value_counts()[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Points must lie on a hyperboloid: Lorentzian Inner Product does not equal the curvature of 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/phil/hdt/src/hyperdt/tree.py\", line 206, in _validate_hyperbolic\n    assert np.allclose(\nAssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py\", line 588, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/phil/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py\", line 588, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/phil/hdt/src/hyperdt/tree.py\", line 239, in fit\n    self._validate_hyperbolic(X)\n  File \"/home/phil/hdt/src/hyperdt/tree.py\", line 212, in _validate_hyperbolic\n    raise ValueError(\"Points must lie on a hyperboloid: Lorentzian Inner Product does not equal the curvature of {}.\".format(self.curvature))\nValueError: Points must lie on a hyperboloid: Lorentzian Inner Product does not equal the curvature of 1.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/phil/hdt/19_hororf.ipynb Cell 39\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m hrf \u001b[39m=\u001b[39m HyperbolicRandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/hdt/19_hororf.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m hrf\u001b[39m.\u001b[39mfit(X, y)\n",
      "File \u001b[0;32m~/hdt/src/hyperdt/forest.py:58\u001b[0m, in \u001b[0;36mRandomForestClassifier.fit\u001b[0;34m(self, X, y, use_tqdm, seed)\u001b[0m\n\u001b[1;32m     56\u001b[0m trees \u001b[39m=\u001b[39m tqdm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees) \u001b[39mif\u001b[39;00m use_tqdm \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m     fitted_trees \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)(\n\u001b[1;32m     59\u001b[0m         delayed(tree\u001b[39m.\u001b[39mfit)(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_subsample(X, y)) \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m trees\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees \u001b[39m=\u001b[39m fitted_trees\n\u001b[1;32m     62\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1939\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1942\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1944\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1586\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1587\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1691\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1685\u001b[0m \n\u001b[1;32m   1686\u001b[0m     \u001b[39m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[39m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[39m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m     \u001b[39m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aborting:\n\u001b[0;32m-> 1691\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1692\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m     \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m     \u001b[39m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:1726\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[39m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[39m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[39m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[39mif\u001b[39;00m error_job \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1726\u001b[0m     error_job\u001b[39m.\u001b[39mget_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:735\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    729\u001b[0m backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_backend\n\u001b[1;32m    731\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    732\u001b[0m     \u001b[39m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[39m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[39m# be returned.\u001b[39;00m\n\u001b[0;32m--> 735\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_or_raise()\n\u001b[1;32m    737\u001b[0m \u001b[39m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/hdt/lib/python3.11/site-packages/joblib/parallel.py:753\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    752\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 753\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[1;32m    754\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[1;32m    755\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Points must lie on a hyperboloid: Lorentzian Inner Product does not equal the curvature of 1."
     ]
    }
   ],
   "source": [
    "from HoroRF.datasets.neuroseed import get_training_data\n",
    "from src.hyperdt.forest import HyperbolicRandomForestClassifier\n",
    "\n",
    "X, y = get_training_data(8, 0, convert_to_poincare=False)\n",
    "X = X.numpy()\n",
    "y = y.numpy()\n",
    "\n",
    "hrf = HyperbolicRandomForestClassifier(n_estimators=1, max_depth=6)\n",
    "hrf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.62415129e-01,  3.02970891e-01,  7.42093042e-01,  3.53212256e-01,\n",
       "       -7.33556736e-02,  4.51581423e-01, -4.65659764e-02,  7.03111294e-01,\n",
       "        3.81061869e-01,  4.19054824e-01,  3.41937011e-01,  4.31299274e-01,\n",
       "        7.78975350e-01,  3.39038215e-01,  4.73590453e-01,  6.87743183e-01,\n",
       "        1.84712044e-01,  1.00699370e-01,  5.92619295e-01, -1.09447602e-01,\n",
       "        3.21642641e-01,  6.29782433e-01,  7.37520295e-01,  5.79525058e-01,\n",
       "        6.19710516e-01,  2.15009685e-01,  5.33383103e-01,  6.46306006e-01,\n",
       "        6.56358909e-01,  8.98726419e-02,  4.96476280e-01,  4.50566421e-01,\n",
       "        7.16286269e-01,  7.00943954e-01,  2.31951404e-01,  7.56062362e-01,\n",
       "        3.00312454e-01,  4.99192027e-01,  2.05736839e-01,  4.90585509e-01,\n",
       "        5.40890290e-01, -9.60216588e-02,  3.71083033e-01,  4.16787794e-01,\n",
       "        4.58929303e-01, -3.05046968e-01,  7.57719097e-01,  5.52870490e-01,\n",
       "        5.69618231e-01,  1.05618678e-01,  7.70246727e-01,  8.66156052e-02,\n",
       "        7.17320870e-01, -2.41078315e-02,  5.00011986e-01,  5.31202491e-01,\n",
       "        5.19943704e-01,  6.96088326e-01,  1.76263020e-01,  7.72586148e-01,\n",
       "        5.15574379e-01,  6.70238332e-01,  5.40281244e-01,  5.67196759e-01,\n",
       "        5.01377054e-01,  4.82610840e-01,  3.41896402e-01,  7.69541916e-01,\n",
       "       -2.67833609e-02,  3.92447056e-02,  5.04847778e-01, -3.78423685e-01,\n",
       "        7.51568377e-01,  6.51042795e-01,  5.94902806e-01,  2.70532577e-01,\n",
       "        7.03994256e-01,  5.12868208e-01,  7.76107755e-01,  7.79065823e-01,\n",
       "       -8.11394083e-02,  7.74532950e-01,  6.84782615e-01,  2.17336783e-01,\n",
       "        7.75407331e-01,  6.13872403e-01,  4.41175978e-01,  7.58049414e-01,\n",
       "        4.03904858e-01,  4.96619052e-01,  6.97625466e-01,  2.51014363e-01,\n",
       "        3.74465698e-01,  5.89695010e-01,  5.15425970e-01,  2.61860729e-01,\n",
       "        4.02291269e-01,  1.61971569e-01,  6.50376719e-01,  6.37768308e-01,\n",
       "        2.79110204e-01,  4.23911671e-01,  4.30670294e-01,  4.76035022e-01,\n",
       "        6.51937467e-01,  4.21380049e-01,  1.65321738e-01,  3.12751098e-01,\n",
       "        5.88650154e-01,  3.73857454e-01,  5.79781461e-01,  5.82550769e-01,\n",
       "        7.40795876e-01,  7.22847635e-01,  6.79212467e-01,  2.64488478e-01,\n",
       "       -2.53284836e-01,  7.71542992e-01,  3.15396577e-01,  6.07306061e-01,\n",
       "        4.67587655e-01,  7.57457314e-01,  5.55124459e-01,  3.63575949e-01,\n",
       "        4.32368866e-01,  1.66908640e-01,  3.92067492e-01,  6.14805547e-01,\n",
       "        5.72542143e-01,  7.36751673e-01,  3.18832661e-02,  6.53737726e-01,\n",
       "        2.35929372e-01,  1.23319898e-01,  7.66331608e-01,  1.66616685e-01,\n",
       "        6.54029416e-01,  7.03251332e-01,  7.13201440e-01,  6.09819052e-01,\n",
       "        3.53594862e-01,  3.93999424e-01,  1.04070877e-02,  3.29830300e-01,\n",
       "        3.90746881e-01,  4.12036423e-01,  5.57302605e-01,  7.86412064e-03,\n",
       "        4.86653204e-01,  5.02324676e-01,  5.81928668e-01,  2.05891985e-01,\n",
       "        6.79959174e-01,  3.34807012e-01,  3.63068432e-01,  4.09519971e-01,\n",
       "        3.64046251e-01,  6.12087569e-01,  6.90908316e-01,  6.96228971e-01,\n",
       "        5.16792269e-01,  4.66481199e-01,  6.62492084e-01,  7.07352910e-01,\n",
       "        2.22365807e-01,  1.02015200e-01,  7.42693333e-01,  1.63997247e-01,\n",
       "        6.09645305e-01,  3.79225266e-01,  3.03042686e-01, -7.44867201e-02,\n",
       "        7.26821748e-01,  3.93152313e-01,  3.77480549e-01,  3.09640010e-02,\n",
       "        7.49355190e-01,  6.49517557e-01, -8.87213188e-02,  3.74741218e-01,\n",
       "        2.91657998e-01,  1.42107153e-01,  7.32270352e-01,  1.81013938e-01,\n",
       "        5.38639726e-01,  2.83758810e-01, -9.38525728e-02,  4.59582983e-01,\n",
       "        5.38963841e-01,  5.19520410e-01,  1.53581343e-01,  4.88121485e-01,\n",
       "        7.67189287e-01,  6.42561124e-01,  3.58863174e-01,  4.04255084e-01,\n",
       "        5.62215032e-01,  3.84415356e-01,  7.50503890e-01,  4.07184767e-01,\n",
       "        6.61160620e-01,  7.04638101e-01,  4.55237946e-01,  4.93364021e-01,\n",
       "        3.95550657e-01,  5.74153369e-01,  4.58567355e-01,  3.43078185e-01,\n",
       "        1.75657381e-01,  3.90408533e-01,  3.08766484e-01, -4.12653098e-01,\n",
       "        1.53078580e-01,  4.97759634e-01,  2.09315191e-01,  4.83074526e-01,\n",
       "        6.70533242e-01,  2.51802284e-01,  6.87949287e-01,  1.82364695e-01,\n",
       "        2.23062527e-01, -5.40062870e-02, -3.91942699e-02, -3.29391004e-04,\n",
       "        4.62821665e-01,  6.22899898e-01, -6.86479446e-02,  5.74310985e-01,\n",
       "        5.35632463e-01,  5.11816766e-01,  5.87498092e-01,  4.02944567e-01,\n",
       "       -4.79477712e-02, -1.79803881e-01,  6.99001688e-01,  5.82587296e-01,\n",
       "        2.00665887e-01,  6.55467689e-01,  4.17313164e-01,  4.11786084e-01,\n",
       "        4.78149519e-01,  7.71052620e-01,  7.56426913e-01,  5.01600555e-01,\n",
       "       -7.61039792e-02,  1.91129858e-02, -2.87251904e-01,  7.78199804e-01,\n",
       "        6.82608207e-01,  2.88478206e-01,  2.38435194e-01, -5.31603166e-02,\n",
       "        4.78140800e-01,  5.17863956e-01,  4.11788451e-01,  4.75791728e-01,\n",
       "       -1.30890642e-03,  5.28941205e-01,  4.39897565e-01,  4.29269863e-01,\n",
       "        3.12013130e-01,  5.27392510e-01,  5.19453325e-01,  4.30388611e-01,\n",
       "        2.70648564e-02,  7.37383994e-01,  7.41345759e-02,  7.06323120e-01,\n",
       "        7.59690214e-01,  7.65254945e-01,  1.60349731e-01,  3.42589085e-01,\n",
       "        4.10087135e-01,  4.78759625e-01,  6.24155281e-01,  1.09934565e-02,\n",
       "        3.45959093e-01,  4.21208478e-01, -8.24999333e-02, -1.07593295e-01,\n",
       "        5.84134548e-01,  6.74383568e-01,  5.07272805e-01,  5.17801533e-01,\n",
       "        6.11329097e-01,  5.98876677e-01,  4.41079604e-01,  5.31431027e-01,\n",
       "       -2.13068947e-01,  7.08802197e-01,  4.49786211e-01,  7.57428132e-01,\n",
       "        3.62627938e-01,  2.21196165e-01,  7.39581554e-01, -1.97905948e-02,\n",
       "        2.57077296e-01,  5.80574193e-02,  2.49243058e-02,  4.27177539e-01,\n",
       "        4.90103052e-02,  5.68008839e-01,  5.97453954e-01,  8.25669479e-02,\n",
       "        1.90502274e-01,  7.26918413e-01,  4.67223304e-01,  6.93434956e-01,\n",
       "       -8.51137884e-02,  4.60847007e-01,  7.17178322e-01,  5.75213446e-01,\n",
       "        3.15330683e-01,  4.66776327e-01, -2.12542936e-01,  5.98559059e-01,\n",
       "        4.27849093e-01,  4.35082761e-01,  2.47955521e-01,  2.42431157e-01,\n",
       "        6.23527407e-01, -2.75620774e-01,  5.28760058e-01, -2.41450716e-01,\n",
       "        3.62265376e-01,  8.05621328e-02,  4.40452878e-01,  3.20001958e-02,\n",
       "        5.97849843e-01,  7.24458782e-01,  4.53320122e-01,  4.32656993e-01,\n",
       "        6.57302651e-01,  7.16102121e-01,  7.33217707e-01,  5.38064011e-01,\n",
       "        7.79069247e-01,  6.40096357e-01, -1.85853910e-02,  3.16302435e-01,\n",
       "        1.16660093e-01,  4.52449534e-01,  6.51008269e-01,  6.08906493e-02,\n",
       "        6.00071160e-01,  3.85119488e-01,  5.50091721e-01,  6.97261023e-01,\n",
       "        3.14791484e-01, -1.90434587e-01,  1.73322895e-01,  3.38371878e-01,\n",
       "        2.11154328e-01,  3.77755329e-01,  3.44299716e-01, -6.60898019e-02,\n",
       "        5.52252204e-01,  5.77863579e-01,  3.58809032e-01,  1.63653367e-01,\n",
       "        5.68448783e-01,  7.17846765e-01,  4.35906211e-01,  6.32648174e-01,\n",
       "        7.56166410e-01,  3.76893286e-01,  3.88318171e-01,  5.35946132e-01,\n",
       "        3.66475227e-01,  5.83124210e-01,  2.09185243e-02,  7.19563047e-01,\n",
       "        5.16018481e-01,  7.89078510e-03,  1.57109532e-01,  6.08768173e-01,\n",
       "        4.84091293e-01,  5.05023292e-01,  2.23713959e-01,  5.26391651e-01,\n",
       "        5.29684143e-01,  7.57239357e-01,  7.07917924e-01,  4.23071493e-01,\n",
       "        4.26323821e-01,  7.47083978e-01, -1.21910811e-01,  1.95080513e-01,\n",
       "       -3.30831389e-02,  4.28595954e-01,  4.80179969e-01,  1.55006170e-01,\n",
       "        4.68587912e-01,  5.47834835e-01,  6.27370158e-01, -1.98526149e-01,\n",
       "        1.03250903e-01,  6.09685892e-01,  3.65781123e-01,  5.91642656e-01,\n",
       "       -2.29269109e-01,  7.77199286e-01,  3.04450441e-01,  7.70313263e-01,\n",
       "        6.73965899e-01, -3.86521377e-02,  2.36288666e-01,  2.57208315e-01,\n",
       "        3.57263024e-01,  6.12909337e-01,  7.47628773e-01, -3.77467578e-01,\n",
       "        2.73503561e-01,  4.06968371e-01,  7.11092444e-01,  6.19304564e-01,\n",
       "        7.79119165e-01,  4.35027474e-01, -1.39916383e-02,  7.39016890e-01,\n",
       "        2.07248978e-01,  5.13979386e-01,  6.99540803e-01,  1.03792688e-01,\n",
       "        7.47879104e-01,  3.65677723e-01,  2.98249921e-01,  4.89637419e-01,\n",
       "        4.37048449e-01,  4.99687637e-01,  5.40443895e-01,  2.80509409e-01,\n",
       "        2.29955427e-01,  5.78336569e-01,  5.84806263e-01,  3.54739266e-01,\n",
       "       -5.23726100e-02,  7.14669646e-01,  4.81547889e-01,  6.70888217e-01,\n",
       "        7.44705446e-01,  5.78089105e-01,  1.11389037e-01,  5.35522079e-01,\n",
       "        7.78712709e-01,  3.44941801e-02,  4.99688915e-01,  3.91113280e-01,\n",
       "        5.67445825e-01,  4.31834781e-01,  4.85559921e-01,  5.07245435e-01,\n",
       "        4.37501819e-01,  1.58071246e-01,  3.91879727e-01,  4.32019764e-01,\n",
       "        5.57974083e-01,  7.61156477e-01,  4.93847360e-01,  4.71871480e-01,\n",
       "       -1.46671580e-02,  4.02459749e-01,  2.69774237e-01, -8.27280570e-02,\n",
       "        6.34906644e-01,  1.32442214e-01,  1.92558785e-01,  7.56876701e-01,\n",
       "        5.67263259e-01,  4.40236127e-01,  4.57603765e-01,  3.44049414e-01,\n",
       "        6.35660805e-01,  3.81619889e-01,  5.19711502e-01,  6.55172378e-01,\n",
       "        5.13000201e-01,  7.57672149e-01,  3.75111796e-01,  7.78315874e-01,\n",
       "        2.96508925e-01,  6.16910774e-01,  7.75686618e-01,  5.51006205e-01,\n",
       "       -3.99110674e-01,  6.81287457e-01,  2.66767431e-01,  7.13296177e-01,\n",
       "        3.31175990e-01,  1.79726925e-01,  8.90106940e-02,  3.92521718e-01,\n",
       "        4.02647624e-01,  7.14477043e-01,  7.67807711e-01,  3.16595648e-01,\n",
       "        6.44050663e-01,  6.79158436e-01,  1.04242906e-01,  3.27291056e-01,\n",
       "        7.38379751e-02,  6.02963244e-01,  7.13715029e-01,  4.69375515e-01,\n",
       "        4.16006960e-01,  7.76472710e-01,  5.41784705e-01,  7.34526019e-01,\n",
       "       -1.26841633e-01,  6.52537887e-01,  5.35833884e-01,  4.43994446e-01,\n",
       "        1.91035194e-01, -1.25494209e-01,  4.80209108e-01,  6.02991954e-01,\n",
       "        5.22382868e-01,  3.43326842e-01, -2.88313000e-01,  7.41643552e-01,\n",
       "        2.79971876e-01,  5.81344475e-01,  3.86671947e-01,  6.14532258e-01,\n",
       "        4.05674348e-01,  6.13872298e-01,  7.39882154e-01,  5.62087317e-01,\n",
       "        6.55978259e-01,  6.86815870e-01,  4.66976044e-01,  7.58179101e-01,\n",
       "        5.47544986e-01,  5.01429217e-01,  5.61675695e-01,  4.52604735e-01,\n",
       "       -2.39767148e-03,  1.21632256e-01,  6.11767089e-01,  5.70361220e-01,\n",
       "        4.57458513e-01,  7.11707154e-01,  6.37785304e-01,  6.37353519e-01,\n",
       "        5.54269043e-01,  6.86524412e-01,  5.17572023e-01,  3.41493665e-01,\n",
       "        4.31778787e-01,  4.41042171e-01,  5.45993142e-01,  2.15534607e-01,\n",
       "        2.69542123e-01,  3.83542240e-01,  4.53467986e-01,  2.27617221e-01,\n",
       "        3.57397300e-01,  2.79674353e-01,  4.96005100e-01,  4.70185675e-01,\n",
       "        1.26413096e-01,  2.14399650e-01,  6.49915010e-01,  7.39962719e-01,\n",
       "        4.99095334e-01,  1.15944745e-01,  4.67652399e-01,  3.69599513e-01,\n",
       "        5.48949505e-01,  2.58122168e-01,  4.00795932e-01,  5.62476881e-01,\n",
       "        4.44056664e-01,  1.07138425e-01,  5.47527037e-01, -6.51325754e-02,\n",
       "        6.15113234e-01, -1.82486974e-01,  7.36923931e-01,  5.52905922e-01,\n",
       "        4.17127368e-01,  2.87808056e-01,  7.78107908e-01,  4.75565178e-01,\n",
       "        5.82291892e-01,  7.63304417e-01,  3.91382640e-01,  4.80882909e-01,\n",
       "        5.18560330e-01,  6.62295683e-01,  5.60166484e-01,  7.62526213e-01,\n",
       "        2.15923863e-01,  1.44220032e-01,  5.50657303e-01,  1.20441323e-01,\n",
       "        3.65615349e-01,  4.73587032e-01,  7.42916058e-01,  2.90884795e-02,\n",
       "        6.18720627e-01,  3.29159409e-01,  5.32213914e-01,  5.47098984e-01,\n",
       "        5.94743414e-01,  6.37972201e-01,  2.46477222e-01,  1.78412446e-01,\n",
       "        6.70928512e-01,  4.60264122e-01,  7.61709154e-01,  7.78382324e-01,\n",
       "        3.55160332e-01,  5.09729784e-01,  5.25752478e-01,  6.25201318e-01,\n",
       "        3.29364126e-01,  5.65350141e-01,  5.31976843e-01,  4.60990630e-01,\n",
       "        5.62159428e-01,  1.34870921e-01,  5.01298184e-01,  6.62633123e-01,\n",
       "        4.02647177e-01,  5.30201046e-01,  3.92824280e-01,  5.41214498e-01,\n",
       "        5.14202579e-01,  5.29884844e-01,  4.13625424e-01,  5.93223246e-01,\n",
       "        2.86095351e-01,  4.47179947e-01,  3.01353753e-01,  2.39361345e-02,\n",
       "        5.86350481e-01,  2.71033475e-02,  4.92316370e-01,  3.08489413e-01,\n",
       "       -2.63209594e-01,  1.88194540e-01,  5.81327170e-01,  7.74502191e-01,\n",
       "        5.58507449e-01,  2.81805736e-01,  2.88741644e-01,  2.01228500e-01,\n",
       "        7.54161785e-01,  1.71167589e-01,  1.32780655e-01,  5.11800829e-01,\n",
       "        6.31508454e-01,  1.52510007e-01,  5.67104344e-01,  3.21508597e-01,\n",
       "        6.26432584e-01,  7.00325940e-01,  3.72044072e-03,  5.33081201e-01,\n",
       "        2.14355623e-01,  6.12884172e-01,  4.06119398e-01,  2.27236794e-01,\n",
       "        1.22209498e-01,  4.78957510e-01,  1.61867376e-01,  4.38291348e-01,\n",
       "        1.53547468e-01,  6.75628510e-01,  6.31033597e-01,  5.59303603e-01,\n",
       "        6.87404832e-01,  2.96452499e-01,  4.73270146e-01,  4.64300959e-01,\n",
       "        6.89578935e-02,  4.72946338e-01,  2.21918869e-01,  1.81372170e-01,\n",
       "        4.75201967e-01,  3.94121424e-01,  5.14498258e-01,  9.23732077e-02,\n",
       "        4.45572588e-01,  5.99578198e-01,  2.17601854e-01,  1.23263032e-01,\n",
       "        4.91594237e-01,  7.62913196e-01,  3.81321198e-01,  6.87420331e-01,\n",
       "        2.09945953e-01,  5.24840038e-01,  7.21727204e-01,  5.97332757e-01,\n",
       "        6.09931642e-01,  6.15503654e-01,  5.99886905e-01,  7.20580640e-01,\n",
       "        6.66394042e-01,  3.42724371e-01,  1.74501024e-01,  4.79434365e-01,\n",
       "        1.30486183e-01,  3.18250230e-01, -9.48205074e-02,  4.91077593e-01,\n",
       "        1.64487344e-01,  7.55044220e-01,  3.70641687e-01,  7.15953699e-01,\n",
       "       -2.53025209e-01,  7.55716682e-01,  6.72951542e-01,  1.79650035e-01,\n",
       "        1.62138149e-02,  7.51346285e-01,  1.54743252e-01,  6.44741405e-01,\n",
       "        7.52637649e-01, -2.77213821e-01,  4.54167964e-01,  4.76666771e-02,\n",
       "        5.66742658e-01,  2.96506800e-01,  5.93194896e-01,  4.96082748e-01,\n",
       "        7.03571300e-01,  2.46327088e-01,  4.09237443e-01, -2.25245575e-02,\n",
       "        2.82274421e-01,  4.60523493e-01,  3.06302085e-01,  6.92307499e-01,\n",
       "        4.26410881e-01,  6.73222806e-01,  2.68938195e-01,  5.07756325e-01,\n",
       "        3.45755538e-01,  6.56350791e-02,  4.20932394e-01,  6.85695982e-01,\n",
       "        7.71940646e-01,  4.26774601e-01,  7.02707640e-01,  6.74436855e-01,\n",
       "        6.42047175e-01,  6.64978264e-01,  6.89244876e-01,  4.05703977e-01,\n",
       "        4.07448108e-01,  6.97880380e-01,  7.39164714e-02,  3.41951366e-01,\n",
       "        7.10648177e-01, -7.09890998e-02,  7.30153944e-01,  7.23712279e-01,\n",
       "       -1.04661729e-01,  4.74979864e-01,  7.38814612e-01,  3.36698650e-01,\n",
       "        4.73959376e-01,  7.11448495e-01,  1.38228580e-01,  4.72131608e-01,\n",
       "        4.78534435e-01,  7.76333015e-01,  2.72645323e-01,  4.43277379e-01,\n",
       "        4.08162506e-01,  7.29055665e-01,  2.80396057e-01,  4.29381443e-01,\n",
       "        5.35321281e-01,  5.28377781e-01,  5.33804102e-01,  5.07682337e-01,\n",
       "        6.05495651e-01,  5.76165053e-01, -3.18213999e-01,  6.66346801e-01,\n",
       "        4.53503478e-01,  3.25623484e-01, -2.32363703e-03,  2.32174955e-01,\n",
       "        5.00920180e-01,  7.22668674e-01,  7.59796736e-01, -3.27473792e-01,\n",
       "        5.73983486e-01,  1.25157628e-01,  4.52178751e-01,  6.72283109e-01,\n",
       "        3.94344200e-01,  1.89666781e-01,  4.20692244e-01,  6.78192847e-01,\n",
       "        6.62558146e-01,  5.41391873e-01,  5.01977565e-01,  5.07695973e-01,\n",
       "        3.64927892e-01,  5.93582299e-01,  9.61650765e-02,  3.54943785e-01,\n",
       "        5.36294678e-01,  8.99437527e-03,  3.41685508e-01, -8.68210540e-02,\n",
       "       -3.77104079e-01,  5.57901140e-01,  1.35602343e-01,  7.68151729e-01,\n",
       "        5.15403102e-01,  3.10035177e-01,  5.74064052e-01, -3.34138786e-01,\n",
       "        6.85255217e-01,  4.41193506e-01,  7.14681489e-01,  4.38490415e-01,\n",
       "        4.94310882e-01,  5.90145364e-01,  1.84810450e-01,  3.85611677e-01,\n",
       "        3.84533779e-01,  4.34125867e-01,  7.25610334e-01,  3.15377884e-01,\n",
       "        4.89850639e-01,  4.16460118e-01, -1.63548901e-01,  4.98966436e-01,\n",
       "        1.91296124e-01,  6.62413639e-02,  7.73992139e-02,  7.06339176e-01,\n",
       "        6.14356367e-01,  5.39134236e-01, -7.37566556e-02,  6.27359207e-01,\n",
       "       -1.79411813e-02,  5.53672705e-01, -3.09543020e-02,  6.45953486e-01,\n",
       "        6.96116701e-01,  7.15688469e-01,  3.26391396e-01,  7.49921799e-01,\n",
       "       -1.11278551e-01,  5.59215022e-01,  3.83006279e-01,  5.56946041e-01,\n",
       "        7.15333649e-01, -4.02917149e-01,  6.72588728e-01,  5.99917306e-01,\n",
       "        1.97725898e-01,  2.65716518e-01,  7.42292333e-01,  2.64587469e-01,\n",
       "        7.49196760e-01,  7.19403349e-01,  2.01809121e-01,  8.49901489e-02,\n",
       "        2.24573688e-01,  6.08821180e-01,  5.64873226e-02,  6.21653663e-01,\n",
       "        3.04672881e-01,  5.25164524e-01,  7.79089298e-01,  4.80650365e-01,\n",
       "        6.71294148e-01,  7.08083058e-01, -1.72576706e-01,  5.36418650e-01,\n",
       "        7.78346793e-01,  5.14518868e-01,  6.62031854e-01,  7.83211893e-02,\n",
       "        2.08416300e-01,  6.57462896e-01,  7.36056720e-01,  5.21502124e-01,\n",
       "        5.20744116e-01,  5.85897511e-01,  1.88702082e-01,  7.63013893e-01,\n",
       "        4.66080505e-01,  5.31094612e-01,  4.23117000e-01,  6.83502108e-01,\n",
       "        2.40335862e-01,  3.22731461e-02,  3.21873744e-01,  7.67065546e-01,\n",
       "        4.88581919e-01,  4.10841151e-01,  5.01966093e-01,  4.47333329e-01,\n",
       "        7.70747612e-01,  2.00177036e-01,  6.45044485e-01,  7.44411189e-01,\n",
       "        7.29500374e-01,  6.79392106e-01,  7.73556803e-01,  5.96047153e-01,\n",
       "        6.48167816e-01,  2.00742796e-01,  6.74602993e-01,  5.23061889e-01,\n",
       "        7.58339209e-01,  5.79749519e-02,  5.17611305e-01,  5.17581116e-01,\n",
       "        7.64452217e-01,  5.71950144e-01,  6.77148481e-01,  5.51243282e-01,\n",
       "        5.24319111e-01,  5.26081756e-01,  4.77332176e-01,  7.42805652e-01,\n",
       "        5.40335734e-01,  5.16084076e-01,  6.83078912e-02,  6.76548117e-01,\n",
       "        7.77100863e-01,  3.78648199e-01,  4.23058166e-01,  5.06636141e-01,\n",
       "        7.05866089e-01,  1.99849733e-01,  4.25320876e-01,  3.91644808e-01,\n",
       "        7.77171648e-01,  1.87185704e-01,  2.16853226e-01,  6.35764133e-01,\n",
       "        5.61934414e-01,  7.73103374e-01,  5.44511248e-01,  6.17186726e-01,\n",
       "        7.69392577e-01,  2.19770919e-01,  4.39052490e-01,  7.27736107e-01,\n",
       "        4.83651289e-01,  5.24600420e-01,  7.79106936e-01,  7.38861224e-01,\n",
       "        3.57117939e-01,  4.11407009e-01,  5.77355630e-01,  6.99305084e-01,\n",
       "        6.63190321e-01,  1.61661368e-01,  6.65236408e-01,  5.63566454e-01,\n",
       "        3.20869299e-01,  7.60572231e-01,  7.20666162e-01,  1.99250107e-01,\n",
       "        4.19069445e-01,  6.17442118e-01,  6.93734066e-01,  7.14124595e-01,\n",
       "        6.51666349e-01,  3.78585706e-01,  5.21961067e-01,  1.86561892e-01,\n",
       "        1.10077811e-02,  3.53232626e-01,  3.45481381e-01,  6.15080183e-01,\n",
       "        5.20319371e-01,  1.57949823e-01,  4.09210873e-01,  6.92364866e-01,\n",
       "        6.27297886e-01,  5.44433463e-01,  6.84258422e-01,  7.78985098e-01,\n",
       "        1.08347455e-01,  5.76402551e-01,  4.48491833e-01,  6.75751481e-01,\n",
       "        8.65207891e-02,  4.17647724e-01,  6.91873363e-01,  1.79436184e-01,\n",
       "        5.34663397e-01,  3.05144046e-01,  8.20920354e-02,  5.29526257e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.sum(X[:, :-1] ** 2, axis=1) - X[:, -1] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-24 {color: black;}#sk-container-id-24 pre{padding: 0;}#sk-container-id-24 div.sk-toggleable {background-color: white;}#sk-container-id-24 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-24 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-24 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-24 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-24 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-24 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-24 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-24 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-24 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-24 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-24 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-24 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-24 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-24 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-24 div.sk-item {position: relative;z-index: 1;}#sk-container-id-24 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-24 div.sk-item::before, #sk-container-id-24 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-24 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-24 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-24 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-24 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-24 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-24 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-24 div.sk-label-container {text-align: center;}#sk-container-id-24 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-24 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-24\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=1, n_estimators=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" checked><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=1, n_estimators=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=1, n_estimators=1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1, max_depth=1)\n",
    "rf.fit(np.random.rand(100000, 2), np.random.randint(0, 3, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-25 {color: black;}#sk-container-id-25 pre{padding: 0;}#sk-container-id-25 div.sk-toggleable {background-color: white;}#sk-container-id-25 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-25 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-25 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-25 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-25 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-25 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-25 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-25 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-25 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-25 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-25 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-25 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-25 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-25 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-25 div.sk-item {position: relative;z-index: 1;}#sk-container-id-25 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-25 div.sk-item::before, #sk-container-id-25 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-25 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-25 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-25 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-25 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-25 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-25 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-25 div.sk-label-container {text-align: center;}#sk-container-id-25 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-25 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-25\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HyperbolicDecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" checked><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HyperbolicDecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>HyperbolicDecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "HyperbolicDecisionTreeClassifier()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.hyperdt.tree import HyperbolicDecisionTreeClassifier\n",
    "from HoroRF.datasets.gaussian import get_training_data\n",
    "from src.hyperdt.conversions import convert\n",
    "\n",
    "hdt = HyperbolicDecisionTreeClassifier(max_depth=0)\n",
    "x, y = get_training_data(2, 0, num_samples=100, convert_to_poincare=False)\n",
    "hdt.fit(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3, 5, 9, 15, 33?\n",
    "# 2^1 + 1, 2^2+1, 2^3+1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
